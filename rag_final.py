# -*- coding: utf-8 -*-
"""RAG_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1skw84ehYaCmrzN-KDC4-MtucGer8sTzf?usp=sharing

# Import of the packages
"""

# Because it is a python file, run manually in the terminal:
# pip install llama-index llama-index-embeddings-huggingface peft auto-gptq optimum bitsandbytes transformers accelerate

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# Loads a Hugging Face embedding model to convert text chunks into dense vector representations
from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex
# Settings manages global config, SimpleDirectoryReader loads documents from a folder, and VectorStoreIndex builds a vector index from them
from llama_index.core.retrievers import VectorIndexRetriever
# Retrieves the most relevant chunks from the vector index based on a query embedding
from llama_index.core.query_engine import RetrieverQueryEngine
# Combines a retriever and a language model to create a RAG pipeline that answers questions
from llama_index.core.postprocessor import SimilarityPostprocessor
# Filters or ranks retrieved documents based on their similarity score to the query

"""# Settings"""

Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
# Sets the embedding model used to convert text into vectors for indexing and retrieval
Settings.llm = None
# Disables the default LLM
Settings.chunk_size = 256
# Defines the number of tokens per text chunk when splitting
Settings.chunk_overlap = 25
# Specifies how many tokens should overlap between consecutive chunks

"""# Article into Vector DB"""

# Import file and os modules for local usage
import os

# Create a folder named "articles" if it doesn't already exist
os.makedirs("articles", exist_ok=True)

# Make sure your documents (e.g., PDF, TXT) are manually placed in the "articles" folder

# Load all documents from the "articles" folder into a list of Document objects
documents = SimpleDirectoryReader("articles").load_data()

# Store into vector DB
index = VectorStoreIndex.from_documents(documents)

"""# Function Search"""

# Specified the number of docs to retreive
top_k = 3

# Retriever
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=top_k,
)

# Builds a query engine that uses the retriever to find relevant chunks and filters them based on a minimum similarity score of 0.5
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],
)

"""# Retrieve the best docs"""

# query documents
query = "What are the two main challenges that hinder the widespread application of the 'LLM-as-a-Judge' approach?"
response = query_engine.query(query)

# Reformats the response by extracting the top_k relevant source nodes and adding their text to build a context for the answer
context = "Context:\n"
for i in range(top_k):
    context = context + response.source_nodes[i].text + "\n\n"
# Show the context to see if it's working as we want
print(context)

"""# Import the LLM we need to generate the response"""

from peft import PeftModel, PeftConfig
# 'peft' is a library for working with models that have been fine-tuned using techniques
# PeftModel allows you to load and apply fine-tuning methods, and PeftConfig manages the configuration for fine-tuning
from transformers import AutoModelForCausalLM, AutoTokenizer
# 'transformers' is a library which simplifies working with pre-trained language models
# AutoModelForCausalLM is used to load a model that generates text
# AutoTokenizer helps in converting text into tokens that the model can understand

# Load a pre-trained language model from Hugging Face Model Hub
model_name = "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
# model_name specifies the specific pre-trained model to load (here, a version of Mistral 7B fine-tuned)

# 'AutoModelForCausalLM.from_pretrained' loads this model and prepares it for generating text
model = AutoModelForCausalLM.from_pretrained(model_name,
                                             device_map=None,  # Run on CPU instead of GPU
                                             trust_remote_code=False,  # Ensures that only trusted code is run for security
                                             revision="main")  # Loads the "main" version of the model (latest one)

# Load the fine-tuning configuration
config = PeftConfig.from_pretrained("shawhin/shawgpt-ft")
# PeftConfig helps load the fine-tuning configuration from the given model. "shawhin/shawgpt-ft" is a specific fine-tuned version

# Apply fine-tuning to the model using the configuration
model = PeftModel.from_pretrained(model, "shawhin/shawgpt-ft")
# This applies the fine-tuned changes from the "shawhin/shawgpt-ft" version to the pre-trained model

# Load the tokenizer for the model (helps convert text into a format the model can process)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
# AutoTokenizer loads a tokenizer that converts regular text into tokens (pieces of words or symbols)
# 'use_fast=True' makes the tokenizer work faster by using optimized methods

"""# Use the LLM with instructions"""

# Instructions for an AI analyzing scientific articles
intstructions_string = f"""You are an AI system designed to analyze and interpret scientific articles. Your responses should focus on providing clear, accurate explanations of scientific concepts and findings, adjusting the level of detail based on the complexity of the article. If necessary, you can explain technical terms in simple language to ensure accessibility while maintaining scientific rigor. Provide insights, summarize key points, and clarify any ambiguities in the article.

Please respond to the following comment.
"""
prompt_template = lambda comment: f'''[INST] {intstructions_string} \n{comment} \n[/INST]'''

# Comment input
comment = "What are the two main challenges that hinder the widespread application of the 'LLM-as-a-Judge' approach?"

# Prompt generation to see what we send to the LLM
prompt = prompt_template(comment)
print(prompt)

# Switch the model to evaluation mode
model.eval()

# The tokenizer converts the input text (the prompt) into a series of numbers called 'input_ids'
# These numbers represent parts of words in a form the model can understand
inputs = tokenizer(prompt, return_tensors="pt")

# Generate text based on the input tokens, limit the output to a maximum of 280 new tokens
outputs = model.generate(input_ids=inputs["input_ids"], max_new_tokens=280)

# Decode the generated token IDs back into human-readable text using the tokenizer to read them
print(tokenizer.batch_decode(outputs)[0])
